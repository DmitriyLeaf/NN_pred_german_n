{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "import codecs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_LEN = 25\n",
    "MAX_LEN_CODE = 123.0\n",
    "END_LEN = 10\n",
    "TRYING = 1\n",
    "F_NAME = \"data_sets/dict.cc_nouns_with_gender.txt\"\n",
    "FILE_TEST_NAME = \"data_sets/deutsch_test_asc_1.txt\"\n",
    "DROP_OUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_name, MAX_LEN_CODE, END_LEN):\n",
    "    training_data_file = open(file_name, 'r')\n",
    "    training_data_list = training_data_file.readlines()\n",
    "    training_data_file.close()\n",
    "\n",
    "    training_data_list = training_data_list[1:]\n",
    "    #print(training_data_list[:10])\n",
    "    np.random.shuffle(training_data_list)\n",
    "    #print(training_data_list[:10])\n",
    "    #input()\n",
    "\n",
    "    f_inputs = [0 for i in range(0, END_LEN)]\n",
    "    targets = []\n",
    "    f_inputs = np.asfarray(f_inputs)\n",
    "\n",
    "    for record in training_data_list:\n",
    "        all_values = record.split(' ')\n",
    "        all_values = [word.replace(\"\\n\", \"\") for word in all_values]\n",
    "\n",
    "        inputs = np.asfarray([ord(letter.lower())/MAX_LEN_CODE for letter in all_values[1]])\n",
    "        if(len(inputs) > END_LEN):\n",
    "            inputs = inputs[len(inputs)-END_LEN:]\n",
    "        elif(len(inputs) < END_LEN):\n",
    "            begin = np.zeros((END_LEN - len(inputs)), dtype=np.float)\n",
    "            inputs = np.append(begin, inputs)\n",
    "        f_inputs = np.vstack((f_inputs, inputs))\n",
    "\n",
    "        targets.append(all_values[0])\n",
    "\n",
    "    f_inputs = np.delete(f_inputs, 0, 0)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(targets)\n",
    "    new_targets = encoder.transform(targets)\n",
    "    #print(new_targets)\n",
    "    f_targets = np_utils.to_categorical(new_targets)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return f_inputs, f_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = readFile(F_NAME, MAX_LEN_CODE, END_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280371 samples, validate on 93457 samples\n",
      "Epoch 1/100\n",
      "280371/280371 [==============================] - 42s 149us/step - loss: 0.9137 - acc: 0.5815 - val_loss: 0.8550 - val_acc: 0.6180\n",
      "Epoch 2/100\n",
      "280371/280371 [==============================] - 44s 158us/step - loss: 0.8278 - acc: 0.6314 - val_loss: 0.7515 - val_acc: 0.6550\n",
      "Epoch 3/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.7322 - acc: 0.6836 - val_loss: 0.6797 - val_acc: 0.7093\n",
      "Epoch 4/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.6643 - acc: 0.7218 - val_loss: 0.6066 - val_acc: 0.7502\n",
      "Epoch 5/100\n",
      "280371/280371 [==============================] - 34s 120us/step - loss: 0.6175 - acc: 0.7440 - val_loss: 0.5690 - val_acc: 0.7642\n",
      "Epoch 6/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.5818 - acc: 0.7616 - val_loss: 0.4997 - val_acc: 0.7953\n",
      "Epoch 7/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.5508 - acc: 0.7794 - val_loss: 0.4850 - val_acc: 0.8055\n",
      "Epoch 8/100\n",
      "280371/280371 [==============================] - 34s 123us/step - loss: 0.5240 - acc: 0.7915 - val_loss: 0.4685 - val_acc: 0.8145\n",
      "Epoch 9/100\n",
      "280371/280371 [==============================] - 34s 122us/step - loss: 0.5042 - acc: 0.8011 - val_loss: 0.4894 - val_acc: 0.8057\n",
      "Epoch 10/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.4848 - acc: 0.8098 - val_loss: 0.4431 - val_acc: 0.8260\n",
      "Epoch 11/100\n",
      "280371/280371 [==============================] - 39s 140us/step - loss: 0.4711 - acc: 0.8161 - val_loss: 0.4475 - val_acc: 0.8239\n",
      "Epoch 12/100\n",
      "280371/280371 [==============================] - 34s 121us/step - loss: 0.4561 - acc: 0.8238 - val_loss: 0.4188 - val_acc: 0.8370\n",
      "Epoch 13/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.4426 - acc: 0.8297 - val_loss: 0.5226 - val_acc: 0.7849\n",
      "Epoch 14/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.4360 - acc: 0.8321 - val_loss: 0.4088 - val_acc: 0.8424\n",
      "Epoch 15/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.4276 - acc: 0.8359 - val_loss: 0.3815 - val_acc: 0.8518\n",
      "Epoch 16/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.4191 - acc: 0.8404 - val_loss: 0.4040 - val_acc: 0.8418\n",
      "Epoch 17/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.4083 - acc: 0.8448 - val_loss: 0.3922 - val_acc: 0.8459\n",
      "Epoch 18/100\n",
      "280371/280371 [==============================] - 35s 124us/step - loss: 0.4030 - acc: 0.8480 - val_loss: 0.4318 - val_acc: 0.8307\n",
      "Epoch 19/100\n",
      "280371/280371 [==============================] - 35s 124us/step - loss: 0.3987 - acc: 0.8490 - val_loss: 0.3551 - val_acc: 0.8682\n",
      "Epoch 20/100\n",
      "280371/280371 [==============================] - 35s 124us/step - loss: 0.3894 - acc: 0.8535 - val_loss: 0.4119 - val_acc: 0.8413\n",
      "Epoch 21/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3832 - acc: 0.8561 - val_loss: 0.3501 - val_acc: 0.8652\n",
      "Epoch 22/100\n",
      "280371/280371 [==============================] - 44s 156us/step - loss: 0.3777 - acc: 0.8581 - val_loss: 0.3763 - val_acc: 0.8534\n",
      "Epoch 23/100\n",
      "280371/280371 [==============================] - 44s 156us/step - loss: 0.3726 - acc: 0.8605 - val_loss: 0.5220 - val_acc: 0.7933\n",
      "Epoch 24/100\n",
      "280371/280371 [==============================] - 47s 167us/step - loss: 0.3686 - acc: 0.8623 - val_loss: 0.3808 - val_acc: 0.8558\n",
      "Epoch 25/100\n",
      "280371/280371 [==============================] - 42s 150us/step - loss: 0.3658 - acc: 0.8642 - val_loss: 0.3464 - val_acc: 0.8658\n",
      "Epoch 26/100\n",
      "280371/280371 [==============================] - 43s 154us/step - loss: 0.3638 - acc: 0.8645 - val_loss: 0.3331 - val_acc: 0.8739\n",
      "Epoch 27/100\n",
      "280371/280371 [==============================] - 43s 155us/step - loss: 0.3575 - acc: 0.8672 - val_loss: 0.3545 - val_acc: 0.8687\n",
      "Epoch 28/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3536 - acc: 0.8693 - val_loss: 0.3233 - val_acc: 0.8799\n",
      "Epoch 29/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3509 - acc: 0.8697 - val_loss: 0.3273 - val_acc: 0.8771\n",
      "Epoch 30/100\n",
      "280371/280371 [==============================] - 43s 153us/step - loss: 0.3466 - acc: 0.8711 - val_loss: 0.3442 - val_acc: 0.8717\n",
      "Epoch 31/100\n",
      "280371/280371 [==============================] - 41s 148us/step - loss: 0.3434 - acc: 0.8731 - val_loss: 0.3750 - val_acc: 0.8549\n",
      "Epoch 32/100\n",
      "280371/280371 [==============================] - 42s 149us/step - loss: 0.3390 - acc: 0.8751 - val_loss: 0.3117 - val_acc: 0.8853\n",
      "Epoch 33/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3382 - acc: 0.8757 - val_loss: 0.3228 - val_acc: 0.8790\n",
      "Epoch 34/100\n",
      "280371/280371 [==============================] - 43s 154us/step - loss: 0.3346 - acc: 0.8775 - val_loss: 0.3506 - val_acc: 0.8653\n",
      "Epoch 35/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3333 - acc: 0.8774 - val_loss: 0.3395 - val_acc: 0.8704\n",
      "Epoch 36/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3294 - acc: 0.8792 - val_loss: 0.3326 - val_acc: 0.8770\n",
      "Epoch 37/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3312 - acc: 0.8782 - val_loss: 0.2981 - val_acc: 0.8884\n",
      "Epoch 38/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3281 - acc: 0.8798 - val_loss: 0.2887 - val_acc: 0.8944\n",
      "Epoch 39/100\n",
      "280371/280371 [==============================] - 41s 147us/step - loss: 0.3253 - acc: 0.8810 - val_loss: 0.3075 - val_acc: 0.8885\n",
      "Epoch 40/100\n",
      "280371/280371 [==============================] - 36s 129us/step - loss: 0.3235 - acc: 0.8818 - val_loss: 0.2868 - val_acc: 0.8952\n",
      "Epoch 41/100\n",
      "280371/280371 [==============================] - 37s 131us/step - loss: 0.3217 - acc: 0.8826 - val_loss: 0.2990 - val_acc: 0.8888\n",
      "Epoch 42/100\n",
      "280371/280371 [==============================] - 43s 153us/step - loss: 0.3180 - acc: 0.8843 - val_loss: 0.3012 - val_acc: 0.8889\n",
      "Epoch 43/100\n",
      "280371/280371 [==============================] - 45s 159us/step - loss: 0.3171 - acc: 0.8847 - val_loss: 0.3262 - val_acc: 0.8795\n",
      "Epoch 44/100\n",
      "280371/280371 [==============================] - 44s 158us/step - loss: 0.3151 - acc: 0.8851 - val_loss: 0.3192 - val_acc: 0.8842\n",
      "Epoch 45/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3136 - acc: 0.8863 - val_loss: 0.3308 - val_acc: 0.8757\n",
      "Epoch 46/100\n",
      "280371/280371 [==============================] - 43s 153us/step - loss: 0.3147 - acc: 0.8856 - val_loss: 0.2931 - val_acc: 0.8948\n",
      "Epoch 47/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3097 - acc: 0.8873 - val_loss: 0.3040 - val_acc: 0.8884\n",
      "Epoch 48/100\n",
      "280371/280371 [==============================] - 45s 160us/step - loss: 0.3098 - acc: 0.8872 - val_loss: 0.3100 - val_acc: 0.8869\n",
      "Epoch 49/100\n",
      "280371/280371 [==============================] - 44s 158us/step - loss: 0.3073 - acc: 0.8887 - val_loss: 0.3976 - val_acc: 0.8483\n",
      "Epoch 50/100\n",
      "280371/280371 [==============================] - 45s 159us/step - loss: 0.3049 - acc: 0.8891 - val_loss: 0.3002 - val_acc: 0.8904\n",
      "Epoch 51/100\n",
      "280371/280371 [==============================] - 46s 163us/step - loss: 0.3035 - acc: 0.8899 - val_loss: 0.3107 - val_acc: 0.8847\n",
      "Epoch 52/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.3006 - acc: 0.8910 - val_loss: 0.2907 - val_acc: 0.8949\n",
      "Epoch 53/100\n",
      "280371/280371 [==============================] - 40s 142us/step - loss: 0.2998 - acc: 0.8913 - val_loss: 0.3007 - val_acc: 0.8901\n",
      "Epoch 54/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.3008 - acc: 0.8916 - val_loss: 0.2925 - val_acc: 0.8954\n",
      "Epoch 55/100\n",
      "280371/280371 [==============================] - 44s 155us/step - loss: 0.2987 - acc: 0.8921 - val_loss: 0.2924 - val_acc: 0.8930\n",
      "Epoch 56/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.2982 - acc: 0.8923 - val_loss: 0.2769 - val_acc: 0.8989\n",
      "Epoch 57/100\n",
      "280371/280371 [==============================] - 44s 157us/step - loss: 0.2965 - acc: 0.8931 - val_loss: 0.2751 - val_acc: 0.9007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "280371/280371 [==============================] - 40s 144us/step - loss: 0.2943 - acc: 0.8937 - val_loss: 0.2862 - val_acc: 0.8957\n",
      "Epoch 59/100\n",
      "280371/280371 [==============================] - 41s 145us/step - loss: 0.2925 - acc: 0.8939 - val_loss: 0.2662 - val_acc: 0.9033\n",
      "Epoch 60/100\n",
      "280371/280371 [==============================] - 35s 125us/step - loss: 0.2929 - acc: 0.8943 - val_loss: 0.2713 - val_acc: 0.9019\n",
      "Epoch 61/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2912 - acc: 0.8948 - val_loss: 0.3180 - val_acc: 0.8828\n",
      "Epoch 62/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2919 - acc: 0.8950 - val_loss: 0.2933 - val_acc: 0.8931\n",
      "Epoch 63/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2895 - acc: 0.8962 - val_loss: 0.2677 - val_acc: 0.9028\n",
      "Epoch 64/100\n",
      "280371/280371 [==============================] - 34s 121us/step - loss: 0.2890 - acc: 0.8955 - val_loss: 0.2915 - val_acc: 0.8961\n",
      "Epoch 65/100\n",
      "280371/280371 [==============================] - 36s 129us/step - loss: 0.2877 - acc: 0.8965 - val_loss: 0.3305 - val_acc: 0.8764\n",
      "Epoch 66/100\n",
      "280371/280371 [==============================] - 35s 123us/step - loss: 0.2875 - acc: 0.8971 - val_loss: 0.3047 - val_acc: 0.8892\n",
      "Epoch 67/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.2868 - acc: 0.8967 - val_loss: 0.3045 - val_acc: 0.8883\n",
      "Epoch 68/100\n",
      "280371/280371 [==============================] - 35s 127us/step - loss: 0.2848 - acc: 0.8977 - val_loss: 0.3174 - val_acc: 0.8797\n",
      "Epoch 69/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2853 - acc: 0.8981 - val_loss: 0.3113 - val_acc: 0.8894\n",
      "Epoch 70/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2811 - acc: 0.8990 - val_loss: 0.3116 - val_acc: 0.8838\n",
      "Epoch 71/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2814 - acc: 0.8989 - val_loss: 0.2787 - val_acc: 0.8984\n",
      "Epoch 72/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2811 - acc: 0.9001 - val_loss: 0.2845 - val_acc: 0.8999\n",
      "Epoch 73/100\n",
      "280371/280371 [==============================] - 34s 123us/step - loss: 0.2771 - acc: 0.9010 - val_loss: 0.2825 - val_acc: 0.8992\n",
      "Epoch 74/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2803 - acc: 0.9002 - val_loss: 0.2943 - val_acc: 0.8942\n",
      "Epoch 75/100\n",
      "280371/280371 [==============================] - 34s 122us/step - loss: 0.2774 - acc: 0.9012 - val_loss: 0.3005 - val_acc: 0.8919\n",
      "Epoch 76/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2796 - acc: 0.9009 - val_loss: 0.2783 - val_acc: 0.9011\n",
      "Epoch 77/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2756 - acc: 0.9016 - val_loss: 0.2661 - val_acc: 0.9078\n",
      "Epoch 78/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2768 - acc: 0.9010 - val_loss: 0.3090 - val_acc: 0.8922\n",
      "Epoch 79/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2764 - acc: 0.9019 - val_loss: 0.2907 - val_acc: 0.8990\n",
      "Epoch 80/100\n",
      "280371/280371 [==============================] - 34s 122us/step - loss: 0.2764 - acc: 0.9011 - val_loss: 0.2741 - val_acc: 0.9015\n",
      "Epoch 81/100\n",
      "280371/280371 [==============================] - 36s 129us/step - loss: 0.2746 - acc: 0.9023 - val_loss: 0.2684 - val_acc: 0.9032\n",
      "Epoch 82/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2756 - acc: 0.9018 - val_loss: 0.2859 - val_acc: 0.8983\n",
      "Epoch 83/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2733 - acc: 0.9028 - val_loss: 0.2811 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2750 - acc: 0.9023 - val_loss: 0.2857 - val_acc: 0.8990\n",
      "Epoch 85/100\n",
      "280371/280371 [==============================] - 36s 127us/step - loss: 0.2727 - acc: 0.9022 - val_loss: 0.2986 - val_acc: 0.8926\n",
      "Epoch 86/100\n",
      "280371/280371 [==============================] - 35s 124us/step - loss: 0.2727 - acc: 0.9034 - val_loss: 0.2868 - val_acc: 0.8967\n",
      "Epoch 87/100\n",
      "280371/280371 [==============================] - 34s 121us/step - loss: 0.2721 - acc: 0.9031 - val_loss: 0.2727 - val_acc: 0.9012\n",
      "Epoch 88/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2715 - acc: 0.9032 - val_loss: 0.2813 - val_acc: 0.8974\n",
      "Epoch 89/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2686 - acc: 0.9045 - val_loss: 0.2600 - val_acc: 0.9092\n",
      "Epoch 90/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2705 - acc: 0.9034 - val_loss: 0.2994 - val_acc: 0.8939\n",
      "Epoch 91/100\n",
      "280371/280371 [==============================] - 36s 129us/step - loss: 0.2723 - acc: 0.9032 - val_loss: 0.3092 - val_acc: 0.8870\n",
      "Epoch 92/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2722 - acc: 0.9038 - val_loss: 0.3036 - val_acc: 0.8945\n",
      "Epoch 93/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2669 - acc: 0.9052 - val_loss: 0.3229 - val_acc: 0.8809\n",
      "Epoch 94/100\n",
      "280371/280371 [==============================] - 35s 124us/step - loss: 0.2696 - acc: 0.9046 - val_loss: 0.2867 - val_acc: 0.8976\n",
      "Epoch 95/100\n",
      "280371/280371 [==============================] - 36s 129us/step - loss: 0.2649 - acc: 0.9062 - val_loss: 0.2878 - val_acc: 0.8978\n",
      "Epoch 96/100\n",
      "280371/280371 [==============================] - 36s 128us/step - loss: 0.2677 - acc: 0.9050 - val_loss: 0.2687 - val_acc: 0.9081\n",
      "Epoch 97/100\n",
      "280371/280371 [==============================] - 35s 126us/step - loss: 0.2669 - acc: 0.9051 - val_loss: 0.2497 - val_acc: 0.9104\n",
      "Epoch 98/100\n",
      "280371/280371 [==============================] - 36s 130us/step - loss: 0.2650 - acc: 0.9060 - val_loss: 0.2534 - val_acc: 0.9113\n",
      "Epoch 99/100\n",
      "280371/280371 [==============================] - 45s 161us/step - loss: 0.2638 - acc: 0.9062 - val_loss: 0.2525 - val_acc: 0.9116\n",
      "Epoch 100/100\n",
      "280371/280371 [==============================] - 43s 154us/step - loss: 0.2653 - acc: 0.9057 - val_loss: 0.3823 - val_acc: 0.8617\n",
      "[2 1 2 0 1 1 2 2 2 2 2 2 1 2 2 1 1 0 2 1 2 2 1 1 1 0 2 1 1 1 2 1 0 2 1 1 1\n",
      " 2 1 1 0 1 2 0 0 1 2 1 0 2 2 1 2 2 1 1 2 2 1 2 2 2 1 1 2 1 1 2 1 0 2 0 1 1\n",
      " 2 2 2 1 1 1 2 1 1 1 2 2 1 0 0 2 1 0 2 1 0 1 2 1 2 2 2 2 2 1 1 0 2 2 2 2 1\n",
      " 1 1 0 1 2 2 2 2 1 1 2 2 2 2 1 0 2 2 2 0 0 2 2 0 1 1 0 1 0 2 2 1 0 1 0 1 0\n",
      " 1 2 2 0 0 2 2 0 0 0 2 1 2 1 0 2 2 1 2 1 0 2 1 0 1 2 2 2 2 1 0 1 1 2 2 1 1\n",
      " 2 1 2 1 0 2 1 1 2 2 1 0 2 0 2 0 0 0 1 2 2 1 1 0 1 1 2 1 1 2 2 1 1 2 2 2 0\n",
      " 2 2 2 1 1 2 2 1 2 1 1 1 2 1 2 2 1 2 2 0 0 1 1 2 2 1 0 2 1 1 2 0 0 2 2 1 2\n",
      " 1 2 0 2 1 2 1 1 2 2 2 2 2 2 1 1 1 0 0 0 1 1 2 0 1 0 2 1 2 2 1 0 1 2 1 0 2\n",
      " 1 0 2 1 1 2 2 2 0 1 1 1 1 1 2 1 1 2 1 1 2 1 2 0 2 0 2 1 2 0 1 0 1 1 2 0 2\n",
      " 2 2 1 2 1 1 2 2 2 1 0 2 1 1 0 1 1 1 2 1 1 1 2 1 2 2 1 1 2 2 2 2 2 2 0 2 2\n",
      " 0 0 1 1 1 2 2 1 0 1 1 2 2 2 2 0 1 1 2 2 2 2 2 1 0 2 2 1 1 0 0 2 2 0 1 1 2\n",
      " 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 0 2 2 1 0 0 1 2 1 2 2 1 0 1 1 2 1 0 2 0 2 2\n",
      " 1 0 2 2 1 1 1 2 1 2 1 0 2 1 0 1 1 2 2 1 2 2 1 0 0 1 1 2 0 2 1 1 1 2 0 1 2\n",
      " 2 1 1 1 2 2 0 2 1 2 2 1 2 0 1 2 2 1 2 1 2 0 2 2 2 2 0 1 2 1 0 2 2 2 2 1 2\n",
      " 1 2 1 0 1 1 2 1 2 2 1 1 1 0 1 1 2 1 2 0 2 1 1 2 2 0 1 0 0 2 0 0 0 1 0 1 2\n",
      " 1 2 1 1 2 0 1 2 2 1 1 0 0 2 1 2 1 0 2 0 1 0 2 2 1 1 1 1 2 2 2 1 0 1 2 0 0\n",
      " 0 2 1 1 0 2 2 1 2 0 2 2 2 2 1 1 2 0 2 1 1 2 1 0 2 2 0 2 1 2 2 1 2 2 2 1 1\n",
      " 0 1 1 2 1 2 0 1 1 1 2 0 1 0 1 2 1 2 1 0 0 0 1 2 1 2 0 2 1 1 2 0 2 0 2 0 1\n",
      " 2 2 1 2 1 1 0 2 0 1 2 0 1 2 1 1 1 2 0 1 1 2 0 1 1 1 1 2 0 1 2 2 1 0 1 1 0\n",
      " 0 0 1 1 2 0 2 2 2 2 2 1 0 1 1 1 1 0 0 0 2 1 2 2 2 0 0 2 2 1 2 0 2 2 0 1 1\n",
      " 0 1 1 1 1 1 1 2 1 2 2 2 2 1 1 2 0 0 1 2 1 1 2 2 2 1 2 1 1 2 0 2 1 1 2 2 2\n",
      " 2 1 1 2 2 2 2 2 0 2 0 2 0 1 0 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 1 0 1 1 0 2 2\n",
      " 0 2 1 0 2 2 1 1 2 2 2 2 2 2 2 1 2 1 0 1 0 2 2 1 2 2 2 2 2 0 1 1 1 1 1 1 0\n",
      " 0 2 1 2 2 1 2 0 1 0 1 1 2 1 1 0 2 1 2 0 2 2 2 1 1 1 2 1 0 2 0 0 2 0 0 1 1\n",
      " 1 1 2 2 1 1 2 2 0 1 2 1 2 0 2 2 1 1 2 2 2 2 2 1 0 2 2 2 2 1 2 2 1 1 0 2 2\n",
      " 2 0 1 2 2 2 2 1 2 1 2 0 1 1 2 2 1 1 2 1 2 0 0 2 2 1 1 0 0 2 1 2 1 1 1 1 2\n",
      " 1 2 2 2 2 1 2 2 1 1 2 0 1 1 1 2 0 0 0 2 1 2 2 1 2 2 1 2 2 1 1 2 1 1 1 2 1]\n",
      "acc: 86.99%\n",
      "[0.3577451566318134, 0.8698698699891985]\n"
     ]
    }
   ],
   "source": [
    "NAME = \"German_noun-8dense-ep100-dataAll-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Dense(END_LEN+20, input_dim=END_LEN, activation='relu'))\n",
    "model.add(layers.Dense(60, activation='relu'))\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(15, activation='relu'))\n",
    "model.add(layers.Dropout(DROP_OUT))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X[1000:], Y[1000:], epochs=100, validation_split=0.25, callbacks=[tensorboard])\n",
    "\n",
    "prediction = model.predict_classes(X[:1000])\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "print(prediction)\n",
    "\n",
    "scores = model.evaluate(X[:1000], Y[:1000], verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [list(i).index(1) for i in Y[:1000]]\n",
    "score = 0\n",
    "for i in range(len(answer)):\n",
    "    if answer[i] == prediction[i]:\n",
    "        score += 1\n",
    "        pass\n",
    "    pass\n",
    "print(score)\n",
    "print(len(answer))\n",
    "result = 100 * score/len(answer)\n",
    "print(\"{}%\".format(round(result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
